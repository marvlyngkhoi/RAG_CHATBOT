{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Document QA System\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "import time\n",
    "import os\n",
    "import groq\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import textract\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your API keys\n",
    "groq_api_key = \"\"\n",
    "pinecone_api_key = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client\n",
    "groq_client = groq.Groq(api_key=groq_api_key)\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"rag-doc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zera/MTP/mtp/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/zera/MTP/mtp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Pinecone index setup\n",
    "cloud = 'aws'\n",
    "region = 'us-east-1'\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "print(\"Initialization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for Text Extraction and Processing\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Input:\n",
    "    - file_path (str): The path to the PDF file.\n",
    "\n",
    "    Output:\n",
    "    - str: The extracted text from the PDF.\n",
    "\n",
    "    This function opens a PDF file, reads all its pages, and extracts the text content.\n",
    "    It concatenates the text from all pages with newline characters between them.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a Word document.\n",
    "\n",
    "    Input:\n",
    "    - file_path (str): The path to the Word document.\n",
    "\n",
    "    Output:\n",
    "    - str: The extracted text from the document.\n",
    "\n",
    "    This function opens a Word document and extracts the text content from all paragraphs,\n",
    "    joining them with newline characters.\n",
    "    \"\"\"\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from various file types.\n",
    "\n",
    "    Input:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Output:\n",
    "    - str: The extracted text from the file.\n",
    "\n",
    "    This function determines the file type based on its extension and uses the appropriate\n",
    "    method to extract text. It supports PDF, DOCX, TXT, and MD files directly, and uses\n",
    "    textract for other file types.\n",
    "    \"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_extension in ['.txt', '.md']:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    else:\n",
    "        return textract.process(file_path).decode('utf-8')\n",
    "\n",
    "def load_data(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Loads and chunks data from a file.\n",
    "\n",
    "    Input:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Output:\n",
    "    - List[str]: A list of text chunks extracted from the file.\n",
    "\n",
    "    This function extracts text from the given file, splits it into paragraphs,\n",
    "    and returns a list of non-empty, stripped text chunks.\n",
    "    \"\"\"\n",
    "    text = extract_text_from_file(file_path)\n",
    "    chunks = text.split('\\n\\n')\n",
    "    return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Gets the embedding for a given text.\n",
    "\n",
    "    Input:\n",
    "    - text (str): The input text to embed.\n",
    "\n",
    "    Output:\n",
    "    - List[float]: The embedding vector as a list of floats.\n",
    "\n",
    "    This function uses the SentenceTransformer model to create an embedding\n",
    "    for the input text. The result is cached to improve performance for repeated calls.\n",
    "    \"\"\"\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "def index_data(chunks: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Indexes the text chunks in Pinecone.\n",
    "\n",
    "    Input:\n",
    "    - chunks (List[str]): A list of text chunks to be indexed.\n",
    "\n",
    "    Output:\n",
    "    - None\n",
    "\n",
    "    This function creates embeddings for each text chunk and upserts them into\n",
    "    the Pinecone index along with their metadata (the original text).\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        vectors.append((str(i), embedding, {\"text\": chunk}))\n",
    "    index.upsert(vectors=vectors)\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def retrieve_relevant_chunks(query: str, top_k: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant chunks for a given query.\n",
    "\n",
    "    Input:\n",
    "    - query (str): The input query.\n",
    "    - top_k (int): The number of top results to retrieve.\n",
    "\n",
    "    Output:\n",
    "    - List[Dict]: A list of dictionaries containing relevant chunks and their metadata.\n",
    "\n",
    "    This function creates an embedding for the query and uses it to find the most\n",
    "    similar chunks in the Pinecone index. The results are cached for performance.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return results['matches']\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def generate_answer(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer for a given query and context.\n",
    "\n",
    "    Input:\n",
    "    - query (str): The input question.\n",
    "    - context (str): The context information to base the answer on.\n",
    "\n",
    "    Output:\n",
    "    - str: The generated answer.\n",
    "\n",
    "    This function creates a prompt using the query and context, sends it to the\n",
    "    Groq language model, and returns the generated answer. Results are cached.\n",
    "    \"\"\"\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processed and indexed successfully!\n"
     ]
    }
   ],
   "source": [
    "## Document Processing and Indexing\n",
    "\n",
    "# Specify the path to your document\n",
    "file_path = \"test.pdf\"  # Change this to your document path\n",
    "\n",
    "# Process and index the document\n",
    "data_chunks = load_data(file_path)\n",
    "index_data(data_chunks)\n",
    "\n",
    "print(\"Document processed and indexed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of this document?\n",
      "Answer: The main topic of this document is the development of an interactive interface for a Question Answering (QA) bot, which allows users to upload documents, ask questions based on the document content, and retrieve real-time answers. The system should handle multiple queries efficiently, provide accurate and contextually relevant responses, and display the retrieved document segments alongside the generated answers. The QA bot is built using a Retrieval-Augmented Generation (RAG) model, a vector database (such as Pinecone DB), and a generative model (like Cohere API) to handle questions related to a provided document or dataset.\n",
      "\n",
      "Retrieved Chunks:\n",
      "\n",
      "Chunk 1 (Score: 0.2055):\n",
      "Part\n",
      "2:\n",
      "Interactive\n",
      "QA\n",
      "Bot\n",
      "Interface\n",
      "Problem\n",
      "Statement:\n",
      "Develop\n",
      "an\n",
      "interactive\n",
      "interface\n",
      "for\n",
      "the\n",
      "QA\n",
      "bot\n",
      "from\n",
      "Part\n",
      "1,\n",
      "allowing\n",
      "users\n",
      "to\n",
      "input\n",
      "queries\n",
      "and\n",
      "retrieve\n",
      "answers\n",
      "in\n",
      "real\n",
      "time.\n",
      "The\n",
      "interface\n",
      "should\n",
      "enable\n",
      "users\n",
      "to\n",
      "upload\n",
      "documents\n",
      "and\n",
      "ask\n",
      "questions\n",
      "based\n",
      "on\n",
      "the\n",
      "content\n",
      "of\n",
      "the\n",
      "uploaded\n",
      "document.\n",
      "Task\n",
      "Requirements:\n",
      "1.\n",
      "Build\n",
      "a\n",
      "simple\n",
      "frontend\n",
      "interface\n",
      "using\n",
      "Streamlit\n",
      "or\n",
      "Gradio\n",
      ",\n",
      "allowing\n",
      "users\n",
      "to\n",
      "upload\n",
      "PDF\n",
      "documents\n",
      "and\n",
      "ask\n",
      "questions.\n",
      "2.\n",
      "Integrate\n",
      "the\n",
      "backend\n",
      "from\n",
      "Part\n",
      "1\n",
      "to\n",
      "process\n",
      "the\n",
      "PDF,\n",
      "store\n",
      "document\n",
      "embeddings,\n",
      "and\n",
      "provide\n",
      "real-time\n",
      "answers\n",
      "to\n",
      "user\n",
      "queries.\n",
      "3.\n",
      "Ensure\n",
      "that\n",
      "the\n",
      "system\n",
      "can\n",
      "handle\n",
      "multiple\n",
      "queries\n",
      "efficiently\n",
      "and\n",
      "provide\n",
      "accurate,\n",
      "contextually\n",
      "relevant\n",
      "responses.\n",
      "4.\n",
      "Allow\n",
      "users\n",
      "to\n",
      "see\n",
      "the\n",
      "retrieved\n",
      "document\n",
      "segments\n",
      "alongside\n",
      "the\n",
      "generated\n",
      "answer.\n",
      "Deliverables:\n",
      "●\n",
      "A\n",
      "deployed\n",
      "QA\n",
      "bot\n",
      "with\n",
      "a\n",
      "frontend\n",
      "interface\n",
      "where\n",
      "users\n",
      "can\n",
      "upload\n",
      "documents\n",
      "and\n",
      "interact\n",
      "with\n",
      "the\n",
      "bot.\n",
      "●\n",
      "Documentation\n",
      "on\n",
      "how\n",
      "the\n",
      "user\n",
      "can\n",
      "upload\n",
      "files,\n",
      "ask\n",
      "questions,\n",
      "and\n",
      "view\n",
      "the\n",
      "bot's\n",
      "responses.\n",
      "●\n",
      "Example\n",
      "interactions\n",
      "demonstrating\n",
      "the\n",
      "bot's\n",
      "capabilities.\n",
      "Guidelines:\n",
      "●\n",
      "Use\n",
      "Docker\n",
      "to\n",
      "containerize\n",
      "the\n",
      "application\n",
      "for\n",
      "easy\n",
      "deployment.\n",
      "●\n",
      "Ensure\n",
      "the\n",
      "system\n",
      "can\n",
      "handle\n",
      "large\n",
      "documents\n",
      "and\n",
      "multiple\n",
      "queries\n",
      "without\n",
      "significant\n",
      "performance\n",
      "drops.\n",
      "●\n",
      "Share\n",
      "the\n",
      "code,\n",
      "deployment\n",
      "instructions,\n",
      "and\n",
      "the\n",
      "final\n",
      "working\n",
      "model\n",
      "through\n",
      "GitHub.\n",
      "General\n",
      "Guidelines:\n",
      "1.\n",
      "Ensure\n",
      "modular\n",
      "and\n",
      "scalable\n",
      "code\n",
      "following\n",
      "best\n",
      "practices\n",
      "for\n",
      "both\n",
      "frontend\n",
      "and\n",
      "backend\n",
      "development.\n",
      "2.\n",
      "Document\n",
      "your\n",
      "approach\n",
      "thoroughly,\n",
      "explaining\n",
      "your\n",
      "decisions,\n",
      "challenges\n",
      "faced,\n",
      "and\n",
      "solutions.\n",
      "3.\n",
      "Provide\n",
      "a\n",
      "detailed\n",
      "ReadMe\n",
      "file\n",
      "in\n",
      "your\n",
      "GitHub\n",
      "repository,\n",
      "including\n",
      "setup\n",
      "and\n",
      "usage\n",
      "instructions.\n",
      "4.\n",
      "Submissions\n",
      "should\n",
      "include:\n",
      "○\n",
      "Source\n",
      "code\n",
      "for\n",
      "both\n",
      "the\n",
      "notebook\n",
      "and\n",
      "the\n",
      "interface.\n",
      "○\n",
      "A\n",
      "fully\n",
      "functional\n",
      "Colab\n",
      "notebook.\n",
      "○\n",
      "Documentation\n",
      "on\n",
      "the\n",
      "pipeline\n",
      "and\n",
      "deployment\n",
      "instructions.\n",
      "\n",
      "Chunk 2 (Score: 0.1906):\n",
      "In conclusion, the stock market is a vital component of the global economy, providing companies with access to capital and investors with the opportunity to earn returns on their investments. It is a complex and dynamic system, driven by a wide range of factors, and it can be challenging for individual investors to stay informed and make informed decisions. To be successful in the stock market, it is essential for investors to understand the different types of stocks and investment options available, to manage risk, and to stay informed about economic indicators and company financials.\n",
      "\n",
      "\n",
      "Chunk 3 (Score: 0.1854):\n",
      "Gen\n",
      "AI\n",
      "Engineer\n",
      "/\n",
      "Machine\n",
      "Learning\n",
      "Engineer\n",
      "Assignment\n",
      "Part\n",
      "1:\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "Model\n",
      "for\n",
      "QA\n",
      "Bot\n",
      "Problem\n",
      "Statement:\n",
      "Develop\n",
      "a\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "model\n",
      "for\n",
      "a\n",
      "Question\n",
      "Answering\n",
      "(QA)\n",
      "bot\n",
      "for\n",
      "a\n",
      "business.\n",
      "Use\n",
      "a\n",
      "vector\n",
      "database\n",
      "like\n",
      "Pinecone\n",
      "DB\n",
      "and\n",
      "a\n",
      "generative\n",
      "model\n",
      "like\n",
      "Cohere\n",
      "API\n",
      "(or\n",
      "any\n",
      "other\n",
      "available\n",
      "alternative).\n",
      "The\n",
      "QA\n",
      "bot\n",
      "should\n",
      "be\n",
      "able\n",
      "to\n",
      "retrieve\n",
      "relevant\n",
      "information\n",
      "from\n",
      "a\n",
      "dataset\n",
      "and\n",
      "generate\n",
      "coherent\n",
      "answers.\n",
      "Task\n",
      "Requirements:\n",
      "1.\n",
      "Implement\n",
      "a\n",
      "RAG-based\n",
      "model\n",
      "that\n",
      "can\n",
      "handle\n",
      "questions\n",
      "related\n",
      "to\n",
      "a\n",
      "provided\n",
      "document\n",
      "or\n",
      "dataset.\n",
      "2.\n",
      "Use\n",
      "a\n",
      "vector\n",
      "database\n",
      "(such\n",
      "as\n",
      "Pinecone\n",
      ")\n",
      "to\n",
      "store\n",
      "and\n",
      "retrieve\n",
      "document\n",
      "embeddings\n",
      "efficiently.\n",
      "3.\n",
      "Test\n",
      "the\n",
      "model\n",
      "with\n",
      "several\n",
      "queries\n",
      "and\n",
      "show\n",
      "how\n",
      "well\n",
      "it\n",
      "retrieves\n",
      "and\n",
      "generates\n",
      "accurate\n",
      "answers\n",
      "from\n",
      "the\n",
      "document.\n",
      "Deliverables:\n",
      "●\n",
      "A\n",
      "Colab\n",
      "notebook\n",
      "demonstrating\n",
      "the\n",
      "entire\n",
      "pipeline,\n",
      "from\n",
      "data\n",
      "loading\n",
      "to\n",
      "question\n",
      "answering.\n",
      "●\n",
      "Documentation\n",
      "explaining\n",
      "the\n",
      "model\n",
      "architecture,\n",
      "approach\n",
      "to\n",
      "retrieval,\n",
      "and\n",
      "how\n",
      "generative\n",
      "responses\n",
      "are\n",
      "created.\n",
      "●\n",
      "Provide\n",
      "several\n",
      "example\n",
      "queries\n",
      "and\n",
      "the\n",
      "corresponding\n",
      "outputs.\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question: str, top_k: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Asks a question and gets an answer based on the indexed document.\n",
    "\n",
    "    Input:\n",
    "    - question (str): The question to ask.\n",
    "    - top_k (int): Number of top chunks to retrieve (default: 3).\n",
    "\n",
    "    Output:\n",
    "    - None (prints the question, answer, and relevant chunks)\n",
    "\n",
    "    This function retrieves relevant chunks from the index, generates an answer\n",
    "    using the language model, and prints the results including the retrieved chunks.\n",
    "    \"\"\"\n",
    "    relevant_chunks = retrieve_relevant_chunks(question, top_k)\n",
    "    context = \"\\n\".join([chunk['metadata']['text'] for chunk in relevant_chunks])\n",
    "    answer = generate_answer(question, context)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"\\nChunk {i+1} (Score: {chunk['score']:.4f}):\")\n",
    "        print(chunk['metadata']['text'])\n",
    "\n",
    "# Example usage\n",
    "ask_question(\"What is the main topic of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
